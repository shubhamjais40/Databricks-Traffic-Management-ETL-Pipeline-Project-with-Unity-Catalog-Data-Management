{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28614893-666c-4ac5-beec-64bc2efefbab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Config catalog widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17249a4-90c1-4937-a920-98fd0606a033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"env\",defaultValue='',label='Enter the environment or catalog in lower case')\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d03d02-4bbb-41e8-8667-98f5b58e5d15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Set path to checkpoints and landing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbda041-b0c2-4bee-9304-421ab949fb66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoints = spark.sql(\"\"\"DESCRIBE EXTERNAL LOCATION `checkpoints`;\"\"\").select('url').collect()[0][0]\n",
    "landing = spark.sql(\" DESCRIBE EXTERNAL LOCATION `landing`;\").select(\"url\").collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4682523-31c9-4f00-a481-2d667cd82d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import all spark dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8c3076-af79-4677-8e60-5c5af5ed861d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4dd4f62-5d14-4e66-be5a-8f3d6b8743c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating a read road_traffic data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c55c69-b4b2-4251-b8f5-45cd6f474651",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_traffic_data():\n",
    "    print(\"Reading the Raw Traffic Data :  \", end='')\n",
    "    schema = StructType([\n",
    "    StructField(\"Record_ID\",IntegerType()),\n",
    "    StructField(\"Count_point_id\",IntegerType()),\n",
    "    StructField(\"Direction_of_travel\",StringType()),\n",
    "    StructField(\"Year\",IntegerType()),\n",
    "    StructField(\"Count_date\",StringType()),\n",
    "    StructField(\"hour\",IntegerType()),\n",
    "    StructField(\"Region_id\",IntegerType()),\n",
    "    StructField(\"Region_name\",StringType()),\n",
    "    StructField(\"Local_authority_name\",StringType()),\n",
    "    StructField(\"Road_name\",StringType()),\n",
    "    StructField(\"Road_Category_ID\",IntegerType()),\n",
    "    StructField(\"Start_junction_road_name\",StringType()),\n",
    "    StructField(\"End_junction_road_name\",StringType()),\n",
    "    StructField(\"Latitude\",DoubleType()),\n",
    "    StructField(\"Longitude\",DoubleType()),\n",
    "    StructField(\"Link_length_km\",DoubleType()),\n",
    "    StructField(\"Pedal_cycles\",IntegerType()),\n",
    "    StructField(\"Two_wheeled_motor_vehicles\",IntegerType()),\n",
    "    StructField(\"Cars_and_taxis\",IntegerType()),\n",
    "    StructField(\"Buses_and_coaches\",IntegerType()),\n",
    "    StructField(\"LGV_Type\",IntegerType()),\n",
    "    StructField(\"HGV_Type\",IntegerType()),\n",
    "    StructField(\"EV_Car\",IntegerType()),\n",
    "    StructField(\"EV_Bike\",IntegerType())\n",
    "    ])\n",
    "\n",
    "    raw_traffic_stream = spark.readStream.format('cloudFiles')\\\n",
    "    .option('cloudFiles.format', 'csv')\\\n",
    "    .option('cloudFiles.schemaLocation',f\"{checkpoints}/rawTrafficLoad/schemaInfer\")\\\n",
    "    .option('header','true')\\\n",
    "    .schema(schema)\\\n",
    "    .load(landing+'/raw_traffic/')\\\n",
    "    .withColumn('Extract_time',current_timestamp())\n",
    "\n",
    "    print(\"Road traffic stream read successful !!\")\n",
    "    return raw_traffic_stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1760d67-3884-40ca-932f-21d59bbd77a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating raw roads read funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b7b491-51ff-494c-8245-97ebf0ffeb10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_road_data():\n",
    "    print(\"Reading the Raw Roads Data :  \", end='')\n",
    "    schema = StructType([\n",
    "        StructField('Road_ID',IntegerType()),\n",
    "        StructField('Road_Category_Id',IntegerType()),\n",
    "        StructField('Road_Category',StringType()),\n",
    "        StructField('Region_ID',IntegerType()),\n",
    "        StructField('Region_Name',StringType()),\n",
    "        StructField('Total_Link_Length_Km',DoubleType()),\n",
    "        StructField('Total_Link_Length_Miles',DoubleType()),\n",
    "        StructField('All_Motor_Vehicles',DoubleType())\n",
    "        ])\n",
    "    raw_road_stream = spark.readStream.format('cloudFiles')\\\n",
    "        .option('cloudFiles.format','csv')\\\n",
    "        .option('cloudFiles.schemaLocation',f\"{checkpoints}/rawRoadsLoad/schemaInfer\")\\\n",
    "        .option('header','true')\\\n",
    "        .schema(schema)\\\n",
    "        .load(landing+'/raw_roads/')\n",
    "    print(\"Raw Road stream read successful !!\")\n",
    "    return raw_road_stream\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83949e56-94b2-4f80-b8ef-5b9193ca056f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating write_traffic_data to bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e1b966-08b7-4238-9208-745454fd7bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Traffic_data(streaming_df,catalog):\n",
    "    print(f'Writing data to {catalog}_catalog raw_traffic table', end='' )\n",
    "    write_Stream = streaming_df.writeStream.format('delta')\\\n",
    "        .option('checkpointLocation',f\"{checkpoints}/rawTrafficLoad/checkpt\")\\\n",
    "        .outputMode('append')\\\n",
    "        .queryName('rawTrafficWriteStream')\\\n",
    "        .trigger(availableNow=True)\\\n",
    "        .toTable(f\"`{catalog}`.`bronze`.`raw_traffic`\")\n",
    "\n",
    "    write_Stream.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb4bd93-c5bc-40e3-a68e-d831e772a56e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating function for raw roads write to bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31aec837-413f-4683-990c-e0329bcea7e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Road_Data(streaming_df,catalog):\n",
    "    print(f'Writing data to {catalog}_catalog raw_roads table', end='' )\n",
    "    write_Data = (streaming_df.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\",f\"{checkpoints}/rawRoadsLoad/checkpt\")\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawRoadsWriteStream')\n",
    "                    .trigger(availableNow=True)\n",
    "                    .toTable(f\"`{catalog}`.`bronze`.`raw_roads`\"))\n",
    "    \n",
    "    write_Data.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521bc719-93ed-46ea-9d1b-ae8e8b4cc41c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calling read and write function to load data from landing to bronze tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89fdd6c-0fe5-4dd3-b343-f413c8c5610d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read traffic data and load to bronze table\n",
    "read_df = read_traffic_data()\n",
    "write_Traffic_data(streaming_df = read_df,catalog = env)\n",
    "\n",
    "read_road = read_road_data()\n",
    "write_Road_Data(streaming_df = read_road,catalog = env)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1000765309533412,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Load To Bronze",
   "widgets": {
    "env": {
     "currentValue": "devops_databricks",
     "nuid": "fb817445-0e18-4898-9407-16c9ef63ec63",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the environment or catalog in lower case",
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter the environment or catalog in lower case",
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
